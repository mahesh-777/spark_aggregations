{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17e71646",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count, countDistinct, approx_count_distinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de2dbc28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7428540a",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.master(\"local\").appName(\"read_df\").config(\"spark.some.config.option\",\"some-value\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4d5c739",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"csv\")\\\n",
    "        .option(\"header\", \"true\")\\\n",
    "        .option(\"inferSchema\", \"true\")\\\n",
    "        .load(r\"D:\\New\\Spark-The-Definitive-Guide\\data\\retail-data\\all\\online-retail-dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fced2415",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[InvoiceNo: string, StockCode: string, Description: string, Quantity: int, InvoiceDate: string, UnitPrice: double, CustomerID: int, Country: string]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a85f7a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"dfTable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d591718",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|count(StockCode)|\n",
      "+----------------+\n",
      "|          541909|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(count(\"StockCode\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a926fe0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "541909"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1474697d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|count(DISTINCT StockCode)|\n",
      "+-------------------------+\n",
      "|                     4070|\n",
      "+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(countDistinct(\"StockCode\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0872279e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+\n",
      "|approx_count_distinct(StockCode)|\n",
      "+--------------------------------+\n",
      "|                            3364|\n",
      "+--------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(approx_count_distinct(\"StockCode\",0.1)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eb0a2055",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import first, last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "196e7373",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---------------+\n",
      "|first(StockCode)|last(StockCode)|\n",
      "+----------------+---------------+\n",
      "|          85123A|          22138|\n",
      "+----------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(first(\"StockCode\"), last(\"StockCode\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "59bdb790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|   InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|12/1/2010 8:26|     2.55|     17850|United Kingdom|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|12/1/2010 8:26|     3.39|     17850|United Kingdom|\n",
      "|   536365|   84406B|CREAM CUPID HEART...|       8|12/1/2010 8:26|     2.75|     17850|United Kingdom|\n",
      "|   536365|   84029G|KNITTED UNION FLA...|       6|12/1/2010 8:26|     3.39|     17850|United Kingdom|\n",
      "|   536365|   84029E|RED WOOLLY HOTTIE...|       6|12/1/2010 8:26|     3.39|     17850|United Kingdom|\n",
      "|   536365|    22752|SET 7 BABUSHKA NE...|       2|12/1/2010 8:26|     7.65|     17850|United Kingdom|\n",
      "|   536365|    21730|GLASS STAR FROSTE...|       6|12/1/2010 8:26|     4.25|     17850|United Kingdom|\n",
      "|   536366|    22633|HAND WARMER UNION...|       6|12/1/2010 8:28|     1.85|     17850|United Kingdom|\n",
      "|   536366|    22632|HAND WARMER RED P...|       6|12/1/2010 8:28|     1.85|     17850|United Kingdom|\n",
      "|   536367|    84879|ASSORTED COLOUR B...|      32|12/1/2010 8:34|     1.69|     13047|United Kingdom|\n",
      "|   536367|    22745|POPPY'S PLAYHOUSE...|       6|12/1/2010 8:34|      2.1|     13047|United Kingdom|\n",
      "|   536367|    22748|POPPY'S PLAYHOUSE...|       6|12/1/2010 8:34|      2.1|     13047|United Kingdom|\n",
      "|   536367|    22749|FELTCRAFT PRINCES...|       8|12/1/2010 8:34|     3.75|     13047|United Kingdom|\n",
      "|   536367|    22310|IVORY KNITTED MUG...|       6|12/1/2010 8:34|     1.65|     13047|United Kingdom|\n",
      "|   536367|    84969|BOX OF 6 ASSORTED...|       6|12/1/2010 8:34|     4.25|     13047|United Kingdom|\n",
      "|   536367|    22623|BOX OF VINTAGE JI...|       3|12/1/2010 8:34|     4.95|     13047|United Kingdom|\n",
      "|   536367|    22622|BOX OF VINTAGE AL...|       2|12/1/2010 8:34|     9.95|     13047|United Kingdom|\n",
      "|   536367|    21754|HOME BUILDING BLO...|       3|12/1/2010 8:34|     5.95|     13047|United Kingdom|\n",
      "|   536367|    21755|LOVE BUILDING BLO...|       3|12/1/2010 8:34|     5.95|     13047|United Kingdom|\n",
      "|   536367|    21777|RECIPE BOX WITH M...|       4|12/1/2010 8:34|     7.95|     13047|United Kingdom|\n",
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "27451a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import min, max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9e95c5bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+\n",
      "|min(Quantity)|max(Quantity)|\n",
      "+-------------+-------------+\n",
      "|       -80995|        80995|\n",
      "+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(min(\"Quantity\"), max(\"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ba358995",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sumDistinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "10ebbf35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mahes\\anaconda3\\envs\\time_series\\lib\\site-packages\\pyspark\\sql\\functions.py:988: FutureWarning: Deprecated in 3.2, use sum_distinct instead.\n",
      "  warnings.warn(\"Deprecated in 3.2, use sum_distinct instead.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+\n",
      "|sum(DISTINCT Quantity)|\n",
      "+----------------------+\n",
      "|                 29310|\n",
      "+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sum_distinct = df.select(sumDistinct(\"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "acd8701c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import var_pop, stddev_pop\n",
    "from pyspark.sql.functions import var_samp, stddev_samp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "12cf223a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+------------------+--------------------+\n",
      "|var_pop(Quantity)|stddev_pop(Quantity)|var_samp(Quantity)|stddev_pop(Quantity)|\n",
      "+-----------------+--------------------+------------------+--------------------+\n",
      "|47559.30364660879|  218.08095663447733| 47559.39140929848|  218.08095663447733|\n",
      "+-----------------+--------------------+------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(var_pop(\"Quantity\"), stddev_pop(\"Quantity\"),\n",
    "         var_samp(\"Quantity\"), stddev_pop(\"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1a17d014",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Although you can calculate average by dividing sum distinct by count.\n",
    "Pyspark provides a beter way to calculate average\"\"\"\n",
    "from pyspark.sql.functions import count, avg, expr, sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f881c875",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------+-----------------+----------------+\n",
      "|(Total_Purchases / Total_Transactions)|Average_Purchases|  Mean_Purchases|\n",
      "+--------------------------------------+-----------------+----------------+\n",
      "|                      9.55224954743324| 9.55224954743324|9.55224954743324|\n",
      "+--------------------------------------+-----------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(sum(\"Quantity\").alias(\"Total_Purchases\"),\n",
    "         count(\"Quantity\").alias(\"Total_Transactions\"),\n",
    "         avg(\"Quantity\").alias(\"Average_Purchases\"),\n",
    "         expr(\"mean(Quantity)\").alias(\"Mean_Purchases\"))\\\n",
    ".selectExpr(\"Total_Purchases/Total_Transactions\",\n",
    "           \"Average_Purchases\",\n",
    "           \"Mean_Purchases\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13d4f0e",
   "metadata": {},
   "source": [
    "## Skewness and Kurtosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2e736687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Both are measurement of the extreme points in the data\n",
    "# Skewness measures the asymmetry of the values in your data around the mean\n",
    "# Kurtosis measures the tail of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d66d0957",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import skewness, kurtosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ee49d3a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+\n",
      "|skewness(Quantity)|kurtosis(Quantity)|\n",
      "+------------------+------------------+\n",
      "|-0.264075576105298|119768.05495534067|\n",
      "+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(skewness(\"Quantity\"), kurtosis(\"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eac3f92",
   "metadata": {},
   "source": [
    "## Covariance and Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "eee1e9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we are trying to measure the interactions of the values in two different columns\n",
    "# Covariance: \n",
    "# Correlation: Measures the pearsons correlatio coefficient which is scaled between -1 and +1.\n",
    "# Covariance: This is scaled according to the inputs in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b32c9971",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import corr, covar_pop, covar_samp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b8703ba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+------------------------------+------------------------------+\n",
      "|corr(Quantity, InvoiceNo)|covar_pop(Quantity, InvoiceNo)|covar_pop(Quantity, InvoiceNo)|\n",
      "+-------------------------+------------------------------+------------------------------+\n",
      "|     4.912186085636769E-4|            1052.7260778751527|            1052.7260778751527|\n",
      "+-------------------------+------------------------------+------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(corr(\"Quantity\", \"InvoiceNo\"), covar_pop(\"Quantity\",\"InvoiceNo\"), covar_pop(\"Quantity\",\"InvoiceNo\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ffd839",
   "metadata": {},
   "source": [
    "## Aggregating to complex types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6f0e3dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aggregating to Complex Types\n",
    "from pyspark.sql.functions import collect_set, collect_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dd1c57c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------------+\n",
      "|collect_set(Country)|collect_list(Country)|\n",
      "+--------------------+---------------------+\n",
      "|[Portugal, Italy,...| [United Kingdom, ...|\n",
      "+--------------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(collect_set(\"Country\"), collect_list(\"Country\")).show()\n",
    "#collect_list: Gives the values in a column in a list\n",
    "#collect_set: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79daff3",
   "metadata": {},
   "source": [
    "## Grouping\n",
    "#### Grouping with expressions, grouping with maps, and just groupBy\n",
    "\n",
    "#### first you have to use the column name whcih you have to groupBy and use the aggregations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5ed397c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+-----+\n",
      "|InvoiceNo|CustomerID|count|\n",
      "+---------+----------+-----+\n",
      "|   536846|     14573|   76|\n",
      "|   537026|     12395|   12|\n",
      "|   537883|     14437|    5|\n",
      "|   538068|     17978|   12|\n",
      "|   538279|     14952|    7|\n",
      "+---------+----------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"InvoiceNo\",\"CustomerID\").count().show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2a81445e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum, col, desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "73e5cefd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+-----+\n",
      "|InvoiceNo|CustomerID|count|\n",
      "+---------+----------+-----+\n",
      "|   576339|     14096|  542|\n",
      "|   579196|     14096|  533|\n",
      "|   580727|     14096|  529|\n",
      "|   578270|     14096|  442|\n",
      "|   573576|     14096|  435|\n",
      "|   567656|     14096|  421|\n",
      "|   567183|     14769|  399|\n",
      "|   575607|     14096|  377|\n",
      "|   571441|     14096|  364|\n",
      "|   570488|     14096|  353|\n",
      "|   572552|     14096|  352|\n",
      "|   568346|     14096|  335|\n",
      "|   547063|     14769|  294|\n",
      "|   569246|     14096|  285|\n",
      "|   562031|     16984|  277|\n",
      "|   554098|     14769|  264|\n",
      "|   570672|     12536|  259|\n",
      "|   543040|     17337|  259|\n",
      "|   569897|     17813|  239|\n",
      "|   572103|     17571|  223|\n",
      "+---------+----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"InvoiceNo\",\"CustomerID\").count().filter(col(\"CustomerID\").isNotNull()).sort(desc(\"count\")).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
